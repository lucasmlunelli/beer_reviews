{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# natural language processing\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\data\\\\'\n",
    "conn = sqlite3.connect('beer.sqlite')\n",
    "\n",
    "# split in 4 dfs due to memory issues\n",
    "df1 = pd.read_sql_query('select * from df1', conn)\n",
    "df2 = pd.read_sql_query('select * from df2', conn)\n",
    "df3 = pd.read_sql_query('select * from df3', conn)\n",
    "df4 = pd.read_sql_query('select * from df4', conn)\n",
    "\n",
    "# close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most commom 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = ['n', 'ratebeer', 'iphone', 'buddy', 'via', 'rated', 'trader']\n",
    "\n",
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "    A simple function to clean up the data. All the words that are not designated as a stop word\n",
    "    are then lemmatized after encoding and basic regex parsing are performed.\n",
    "    \"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "def ngram(df):\n",
    "    \"\"\"\n",
    "    Find most commom 3-gram for each beer.\n",
    "    \"\"\"\n",
    "    # remove beers with less than 50 reviews\n",
    "    df_filter = df.groupby(['beer_id']).agg(count = ('rating', 'count')).reset_index()\n",
    "    df_filter = df_filter[df_filter['count']>50]\n",
    "    df_filter = pd.merge(df_filter, df, on='beer_id')\n",
    "\n",
    "    users = []\n",
    "    beers = df_filter['beer_id'].drop_duplicates().to_list()\n",
    "    for beer in beers:\n",
    "        beer_id = df_filter[df_filter['beer_id']==beer]\n",
    "        words = basic_clean(''.join(str(beer_id['text'].tolist())))\n",
    "        ngram = pd.Series(nltk.ngrams(words, 3)).value_counts().to_frame(name='count').reset_index()\n",
    "        ngram = ngram[ngram['count']>=10] # keep only ngrams that appear more than 10 times\n",
    "        ngram['beer_id'] = beer\n",
    "        users.append(ngram)\n",
    "    return pd.concat(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams1 = ngram(df1)\n",
    "ngrams2 = ngram(df2)\n",
    "ngrams3 = ngram(df3)\n",
    "ngrams4 = ngram(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join data\n",
    "ngrams = pd.concat([ngrams1, ngrams2, ngrams3, ngrams4])\n",
    "\n",
    "# keep only top 20 ngram for each beer\n",
    "ngrams.reset_index(inplace=True)\n",
    "ngrams = ngrams[ngrams['level_0']<20]\n",
    "\n",
    "# clean and prepare final data\n",
    "ngrams['index'] = ngrams['index'].astype(str)\n",
    "ngrams['word'] = ngrams['index'].str.replace(',', '').str.replace('(', '').str.replace(')', '').str.replace(\"'\", '')\n",
    "ngrams['word'] = ngrams['word'].str.replace(' head aroma', ' head').str.replace('head aroma ', 'aroma ')\n",
    "ngrams = ngrams[['beer_id', 'count', 'word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find adjectives (very time consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_STOPWORDS =['clear','white','text','golden','yellow','pale','mexican','much',\n",
    "                       'dark','bottle','brown','black','green','Indian','deep','tap','colour',\n",
    "                       'reddish','canadian','offwhite','red','many','first','overall','large','clean',\n",
    "                       'aroma','american','ale','low','amber','cold','japanese','high','lime','ligh','full',\n",
    "                       'crystal','brownish','know','smal','Name','gold','head','beige','available','Second',\n",
    "                       'isnt','malt','cant','tall','thi','une','several','middle','lemon','bottled','true',\n",
    "                       'lager','initial','macro','drank','finish','brew','caramel','single','english','dont','close',\n",
    "                       'dtype']\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))+ ADDITIONAL_STOPWORDS\n",
    "allowed_word_types = [\"JJS\", 'JJ']\n",
    "\n",
    "def adj(df):\n",
    "    # remove beers with less than 50 reviews\n",
    "    df_filter = df.groupby(['beer_id']).agg(count = ('rating', 'count')).reset_index()\n",
    "    df_filter = df_filter[df_filter['count']>50]\n",
    "    df_filter = pd.merge(df_filter, df, on='beer_id')\n",
    "    df_filter.reset_index(drop=True, inplace =True)\n",
    "    df_filter.reset_index(inplace =True)\n",
    "    index = df_filter['index'].to_list()\n",
    "    \n",
    "    all_words = []\n",
    "\n",
    "    for p in index:\n",
    "\n",
    "        # remove punctuations\n",
    "        cleaned = re.sub(r'[^(a-zA-Z)\\s]','', str(df_filter[df_filter['index']==p]['text']).lower())\n",
    "\n",
    "        # tokenize \n",
    "        tokenized = word_tokenize(cleaned)\n",
    "\n",
    "        # remove stopwords \n",
    "        stopped = [w for w in tokenized if not w in stop_words]\n",
    "\n",
    "        # parts of speech tagging for each word \n",
    "        pos = nltk.pos_tag(stopped)\n",
    "        pos_df = pd.DataFrame(pos ,columns=['word', 'type'])\n",
    "\n",
    "        # make a list of  all adjectives identified by the allowed word types list above\n",
    "        pos_df = pos_df[pos_df['type'].isin(allowed_word_types)]\n",
    "        pos_df['pos'] = p\n",
    "        pos_df.set_index('pos', inplace = True)\n",
    "        pos_df = pos_df.join(df_filter[['beer_id']])\n",
    "        all_words.append(pos_df)\n",
    "\n",
    "    return pd.concat(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj1 = adj(df1)\n",
    "adj2 = adj(df2)\n",
    "adj3 = adj(df3)\n",
    "adj4 = adj(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join data\n",
    "adj = pd.concat([adj1,adj2,adj3,adj4])\n",
    "\n",
    "# group by beer\n",
    "adj_agg = adj.groupby(['beer_id', 'word']).agg(count = ('word', 'count')).reset_index()\n",
    "\n",
    "# remove words that are not common\n",
    "adj_agg = adj_agg[adj_agg['count']>=5]\n",
    "\n",
    "# remove words with less than 3 letters\n",
    "adj_agg = adj_agg[adj_agg['word'].str.len()>2]\n",
    "\n",
    "# remove words that doesnt have a good meaning alone\n",
    "remove = ['small','thin','object','big','pour','nose','little','name','huge','poured','belgian','ive','ipa',\n",
    "          'imperial','total','double','last','lambic','russian','tiny','massive','german','southern','triple',\n",
    "          'trappist','scottish','danish','irish']\n",
    "\n",
    "adj_agg = adj_agg[~adj_agg['word'].isin(remove)]\n",
    "\n",
    "# remove words outside top 30 for each beer\n",
    "adj_agg['rank'] = adj_agg.groupby('beer_id')['count'].rank(ascending=False)\n",
    "adj_agg = adj_agg[adj_agg['rank']<30]\n",
    "\n",
    "adj_agg = adj_agg[['beer_id', 'count', 'word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join adjectives and ngrams and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.concat([ngrams, adj_agg])\n",
    "review.to_csv('beer review.csv', index=False, encoding='utf-8', decimal=',', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
